<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Simone Bendazzoli | MINNE lab</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Simone Bendazzoli" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MINNE lab: Research in biomedical imaging and deep learning" />
<meta property="og:description" content="MINNE lab: Research in biomedical imaging and deep learning" />
<link rel="canonical" href="http://localhost:4000/people/simone" />
<meta property="og:url" content="http://localhost:4000/people/simone" />
<meta property="og:site_name" content="MINNE lab" />
<meta property="og:image" content="http://localhost:4000/assets/people/simben.jpg" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/assets/people/simben.jpg" />
<meta property="twitter:title" content="Simone Bendazzoli" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"MINNE lab: Research in biomedical imaging and deep learning","headline":"Simone Bendazzoli","image":"http://localhost:4000/assets/people/simben.jpg","url":"http://localhost:4000/people/simone"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="MINNE lab" /></head>
<body><header class="site-header" role="banner">
  <div class="wrapper"><div class="site-title">
      <a rel="author" href="/">MINNE <span id="title-ai">lab</span><br/></a>
      <div class="site-subtitle">at KTH Royal Institute of Technology</div>
    </div>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger"><a class="page-link" href="/people/">PEOPLE</a><a class="page-link" href="/updates/">UPDATES</a><a class="page-link" href="/code/">CODE</a><a class="page-link" href="/COURSES/">COURSES</a><a class="page-link" href="/maia/">MAIA</a><a class="page-link" href="/join-us/">JOIN US</a></div>
    </nav>

  </div>
</header>
<main class="page-content" aria-label="Content">    
      <div class="wrapper">
  <article class="post">

    <header class="post-header">
      <h1 class="post-title">Simone Bendazzoli</h1>
    </header>

    <div class="post-content">
      <p><img src="/assets/people/simben.jpg" alt="Simone" class="people-profile-image" /></p>

<h2 id="about-me">About me</h2>

<p>I began my PhD in October 2020, focusing on medical image processing using deep learning — a field that has rapidly evolved but still faces challenges in real-world clinical adoption. Over the years, working at the intersection of AI research and medical practice has made me reflect deeply on how difficult it remains to integrate the models we develop into clinical workflows in a transparent, reproducible, and scalable way.</p>

<p>This realization led me to design and build <a href="/maia/">MAIA</a>, an open-source Medical AI platform that promotes collaboration and reproducibility in medical imaging research. MAIA integrates Kubernetes, MONAI, and the MONAI Bundle format to simplify deploying, configuring, and updating AI models in distributed research environments. It integrates essential tools such as 3D Slicer, OHIF Viewer, Orthanc, XNAT, and MLFlow, forming a cohesive ecosystem for annotation, model training, active learning, and refinement.</p>

<p>Through this work, I aim to bridge the gap between AI innovation and clinical implementation by providing researchers and clinicians with the infrastructure they need to collaborate effectively.</p>

<p>As I approach the defense of my PhD thesis on November 27, 2025, my focus remains on advancing open, modular, and transparent medical AI systems that can empower the community and accelerate the safe translation of research into practice.</p>

<p>When I’m not coding or experimenting with new infrastructure designs, I’m usually out running — my favorite way to clear my mind, stay focused, and find inspiration for the next challenge. Follow my running activities on Strava:</p>
<p align="center">
  <a href="https://www.strava.com/athletes/107384738">
    <img src="/assets/socials/strava_black.png" alt="Strava" width="40" />
    Simone Bendazzoli - Strava
  </a>
</p>

<h2 id="research">Research</h2>

<h3 id="maia-a-collaborative-medical-ai-platform-for-integrated-healthcare-innovation">MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation</h3>

<p><strong>Simone Bendazzoli</strong>, Sanna Persson, Mehdi Astaraki, Sebastian Pettersson, Vitali Grozman, Rodrigo Moreno<br />
<em>KTH Royal Institute of Technology,  Karolinska University Hospital &amp; Karolinska Institutet, Stockholm, Sweden</em></p>

<p>The integration of <strong>Artificial Intelligence (AI)</strong> into clinical workflows requires robust platforms that bridge the gap between technical innovation and practical healthcare applications. <strong>MAIA (Medical Artificial Intelligence Assistant)</strong> is an <strong>open-source platform</strong> designed to facilitate interdisciplinary collaboration among clinicians, researchers, and AI developers.</p>

<p>Built on <strong>Kubernetes</strong>, MAIA provides a <strong>modular, scalable environment</strong> with integrated tools for:</p>
<ul>
  <li>Data management</li>
  <li>Model development</li>
  <li>Annotation and active learning</li>
  <li>Deployment and clinical feedback</li>
</ul>

<p>Key features include <strong>project isolation</strong>, <strong>CI/CD automation</strong>, and integration with <strong>high-computing infrastructures</strong> and real-world clinical workflows. MAIA supports deployments in both academic and clinical environments, enabling reproducible, transparent, and user-centered AI development.</p>

<p>MAIA has been applied in multiple medical imaging projects at <strong>KTH Royal Institute of Technology</strong> and <strong>Karolinska University Hospital</strong>, demonstrating its ability to accelerate the translation of AI research into impactful clinical solutions.</p>

<ul>
  <li><strong>Preprint:</strong> <a href="https://arxiv.org/abs/2507.19489">https://arxiv.org/abs/2507.19489</a></li>
  <li><strong>More details:</strong> <a href="/maia/">MAIA page</a></li>
  <li><img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" alt="GitHub" width="20" style="vertical-align:middle" /> <strong>GitHub Code:</strong> <a href="https://github.com/minnelab/MAIA">MAIA</a></li>
</ul>

<hr />

<h3 id="monet-bundle-integrating-nnu-net-within-the-monai-ecosystem">MONet Bundle: Integrating nnU-Net within the MONAI Ecosystem</h3>

<p><strong>MONet</strong> is a <strong>nnU-Net-based MONAI Bundle</strong> designed to bridge the gap between <strong>state-of-the-art medical image segmentation research</strong> and <strong>real-world clinical deployment</strong>. While nnU-Net is widely recognized for its robustness, versatility, and self-configuring capabilities across diverse medical imaging tasks, its native implementation faces challenges in clinical settings, such as limited portability, adaptability, and integration with existing hospital workflows.</p>

<p>The <strong>MONAI ecosystem</strong> provides a standardized framework for training, deployment, and annotation of medical AI models, including tools like <strong>MONAI Deploy</strong> for clinical integration and <strong>MONAI Label</strong> for AI-assisted image annotation. Starting from MONAI v1.2.0, <strong>nnU-Net</strong> has been partially integrated via <code class="language-plaintext highlighter-rouge">nnUNetV2Runner</code>, enabling data preparation, preprocessing, training, and cross-validation within MONAI pipelines. However, deployment and packaging for diverse clinical applications remained limited.</p>

<p><strong>MONet Bundle</strong> extends <code class="language-plaintext highlighter-rouge">nnUNetV2Runner</code> to:</p>
<ul>
  <li>Package nnU-Net models into <strong>versatile bundles</strong> for multiple tasks.</li>
  <li>Enable <strong>federated learning</strong> and <strong>multi-center collaboration</strong>.</li>
  <li>Integrate with <strong>clinical workflows</strong>, improving portability and adaptability.</li>
</ul>

<p>This ongoing effort contributes to both <strong>nnU-Net</strong> and <strong>MONAI repositories</strong> (MONAI Core, MONAI Deploy, and MONAI Label), providing a <strong>scalable and reproducible framework</strong> that facilitates the transition of segmentation models from research to real-world clinical use.</p>

<p><strong>Applications and Achievements:</strong></p>
<ul>
  <li>Competed in <strong>BraTS 2025 Challenge Task 07: GoAT – Generalizability Across Tumors</strong>, where MONet Bundle achieved <strong>1st prize</strong>, demonstrating robust performance across adult glioma, brain metastasis, meningioma, pediatric glioma, and sub-Saharan African cohorts.</li>
  <li>
    <p>Used in a <strong>federated learning approach for whole-body PET-CT lymphoma segmentation</strong>, presented at the <strong>DeCaF 2025 Workshop</strong> at MICCAI. <a href="https://link.springer.com/chapter/10.1007/978-3-032-05663-4_10">Paper link</a></p>
  </li>
  <li><img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" alt="GitHub" width="20" style="vertical-align:middle" /> <strong>Code:</strong> <a href="https://github.com/minnelab/MONet-Bundle">https://github.com/minnelab/MONet-Bundle</a></li>
  <li><img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/readthedocs.svg" alt="Documentation" width="20" style="vertical-align:middle" /> <strong>Documentation:</strong> <a href="https://maia-toolkit.readthedocs.io">https://minnelab.github.io/maia/</a></li>
</ul>

<p><img src="/assets/MONet_Bundle.png" alt="MONet Bundle" /></p>

<hr />

<h3 id="maia-segmentation-portal">MAIA Segmentation Portal</h3>

<p>The <strong>MAIA Segmentation Portal</strong> is a user-friendly platform hosted within <strong>MAIA</strong>, allowing users to explore and interact with available medical AI models. Users can upload medical images and receive predictions almost instantly, or download the models to run locally if preferred.</p>

<p>The portal leverages models in the <strong>MONet Bundle format</strong>, which are compatible with <strong>MONAI Deploy</strong> and <strong>MONAI Label</strong>, making it easy to integrate AI into clinical workflows. Each model is deployed as a standalone application within MAIA, and the portal is built on a <strong>KubeFlow-based infrastructure</strong>, allowing smooth interaction and inference directly from the portal interface.</p>

<p>The MAIA Segmentation Portal also provides guidance for users who want to <strong>train their own models</strong> using their annotated data, promoting reproducibility and collaboration in medical AI research.</p>

<ul>
  <li><img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/readthedocs.svg" alt="Documentation" width="20" style="vertical-align:middle" /> <strong>Documentation:</strong> <a href="https://monet-bundle.readthedocs.io/en/latest/MAIA_Segmentation_Portal.html">MAIA Segmentation Portal Documentation</a></li>
</ul>

<p><img src="/assets/MAIA_Segmentation_Portal.png" alt="MAIA Segmentation Portal" /></p>

<hr />
<h3 id="lung-vessel-connectivity-map-as-anatomical-prior-knowledge-for-deep-learningbased-lung-lobe-segmentation">Lung Vessel Connectivity Map as Anatomical Prior Knowledge for Deep Learning–Based Lung Lobe Segmentation</h3>
<p><strong>Simone Bendazzoli</strong>, Emelie Bäcklin, Örjan Smedby, Birgitta Janerot-Sjöberg, Bryan Connolly, Chunliang Wang<br />
<em>KTH Royal Institute of Technology, Karolinska Institutet, Karolinska University Hospital</em><br />
<em>J. Med. Imaging, 2024 — <a href="https://pubmed.ncbi.nlm.nih.gov/38988990/">PMID: 38988990</a> | <a href="https://doi.org/10.1117/1.JMI.11.4.044001">DOI: 10.1117/1.JMI.11.4.044001</a></em></p>

<p>This study explores how <strong>anatomical priors</strong> can enhance deep learning segmentation of lung lobes from chest CT. We introduced a <strong>Lung Vessel Connectivity (LVC) map</strong>, encoding the vascular structure of the lungs, and integrated it into three <strong>nnU-Net</strong> variants: standard, multitask, and cascade models.</p>

<p>Including the LVC map improved segmentation accuracy, especially at challenging interlobar boundaries and in expiration scans, while also improving <strong>robustness in pathological cases</strong> such as COVID-19. The results demonstrate that embedding anatomical knowledge into model design can enhance both <strong>accuracy and generalization</strong>, promoting more <strong>clinically reliable</strong> segmentation systems</p>

<hr />

<h3 id="designing-radio-dynamics-features-for-pcr-prediction-in-breast-dce-mri">Designing Radio-dynamics Features for pCR Prediction in Breast DCE-MRI</h3>
<p><strong>Simone Bendazzoli</strong>, Mehdi Astaraki, Yanbo Li, Rodrigo Moreno, Örjan Smedby, Hong Lu, Chunliang Wang<br />
<em>KTH Royal Institute of Technology, Karolinska Institutet, Tianjin Medical University Cancer Hospital</em></p>

<p>This work introduces a novel family of imaging biomarkers, <strong>radio-dynamics features</strong>, designed to improve <strong>pathological complete response (pCR)</strong> prediction in breast cancer patients undergoing <strong>neoadjuvant chemotherapy (NAC)</strong>. These features are derived from <strong>dynamic contrast-enhanced MRI (DCE-MRI)</strong> and aim to capture the <strong>temporal dynamics</strong> of tumor enhancement beyond what traditional radiomics can represent.</p>

<p>The study implemented a full machine learning pipeline for <strong>feature extraction, selection, and classification</strong>, evaluated on 80 subjects using 5-fold cross-validation. Results showed that radio-dynamics features outperformed conventional radiomics, particularly in ensemble models, demonstrating their potential for more accurate and physiologically meaningful prediction of treatment response.</p>

<p>The <strong>source code and pipeline</strong> are openly available at: <a href="https://github.com/minnelab/Hive_ML">https://github.com/minnelab/Hive_ML</a></p>


    </div>

  </article>
</div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <!--<h2 class="footer-heading">MINNE lab</h2>-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <!---->
        <!--
        <ul class="contact-list"><li>Contact:</li>
          <li><a class="u-email" href="mailto:rodmore@kth.se">rodmore@kth.se</a></li></ul>-->
      </div>

      <div class="footer-col footer-col-2">

      </div>
    </div>

  </div>

</footer>
</body>

</html>
